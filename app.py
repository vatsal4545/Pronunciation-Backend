from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import os
import time
import uuid
import requests
import tempfile
import random
# import soundfile as sf
import numpy as np
import openai
# Try to import and initialize Azure Speech SDK with comprehensive error handling
try:
    import azure.cognitiveservices.speech as speechsdk
    print("=== Azure Speech SDK imported successfully ===")
    
    # Enable SDK for advanced features (content assessment, prosody, etc.)
    print("=== Azure Speech SDK enabled for advanced pronunciation assessment ===")
    AZURE_SDK_AVAILABLE = True
        
except ImportError as import_error:
    print(f"=== Azure Speech SDK import failed: {import_error} ===")
    print("=== Will use Azure REST API only ===")
    speechsdk = None
    AZURE_SDK_AVAILABLE = False
except Exception as general_error:
    print(f"=== Azure Speech SDK general error: {general_error} ===")
    print("=== Will use Azure REST API only ===")
    speechsdk = None
    AZURE_SDK_AVAILABLE = False

print(f"=== FINAL AZURE_SDK_AVAILABLE: {AZURE_SDK_AVAILABLE} ===")
from dotenv import load_dotenv
from werkzeug.utils import secure_filename
import json
import base64
import wave
from flask import send_file

# Load environment variables from .env file
load_dotenv()

# Configuration
PRONUNCIATION_THRESHOLD = 75  # Words with scores below this will be marked as mispronounced (adjustable: 60-90)

# Initialize Flask app
app = Flask(__name__)
CORS(app)  # Enable CORS for all routes

# Initialize API keys
openai.api_key = os.getenv("OPENAI_API_KEY")
AZURE_SPEECH_KEY = os.getenv("AZURE_SPEECH_KEY")
AZURE_SPEECH_REGION = os.getenv("AZURE_SPEECH_REGION")
AZURE_SPEECH_ENDPOINT = os.getenv("AZURE_SPEECH_ENDPOINT")

# Session storage (in-memory for demo)
sessions = {}

# Topics and prompts
TOPICS = {
    "Business": "Let's discuss business strategies and professional development.",
    "Technology": "Let's talk about the latest technology trends and innovations.",
    "Interview Roleplay": "I'll be the interviewer. Tell me about your background and skills.",
    "Dating Roleplay": "Let's practice casual conversation as if we're on a first date.",
    "Client Meeting": "Let's roleplay a client meeting. I'll be the client interested in your services."
}

# Initialize the conversation with system prompt
SYSTEM_PROMPT = """
You are an AI pronunciation tutor named Speak Spark. Your job is to engage users in natural conversation while helping them improve their pronunciation and grammar. Follow these guidelines:

1. Keep your responses conversational and engaging but relatively brief (2-3 sentences).
2. When you receive pronunciation feedback, incorporate it naturally into your response.
3. If pronunciation scores are below 75%, gently suggest practicing specific words.
4. Provide specific, constructive feedback on pronunciation and grammar in a encouraging way.
5. Use phrases like "I noticed..." or "Let's work on..." when giving feedback.
6. Stay on the topic the user has selected while weaving in pronunciation tips.
7. Ask open-ended questions to keep the conversation flowing.
8. Celebrate improvements and progress to keep users motivated.
9. If a user struggles with the same word repeatedly, offer to practice it specifically.
10. Make pronunciation learning feel natural and not overwhelming.
"""

# @app.route('/api/generate_speech', methods=['POST'])
# def generate_speech_endpoint():
#     data = request.json
#     text = data.get('text', '')
#     session_id = data.get('session_id', '')
    
#     if not text:
#         return jsonify({'error': 'No text provided'}), 400
    
#     try:
#         # Generate speech using Azure TTS
#         speech_file_path = generate_speech(text)
        
#         return jsonify({
#             'audio_url': '/api/get_audio/' + os.path.basename(speech_file_path)
#         })
    
#     except Exception as e:
#         return jsonify({'error': str(e)}), 500



@app.route('/api/generate_speech', methods=['POST'])
def generate_speech_endpoint():
    data = request.json
    text = data.get('text', '')
    session_id = data.get('session_id', '')
    
    if not text:
        return jsonify({'error': 'No text provided'}), 400
    
    try:
        print(f"Generating speech for: {text[:50]}...")
        print(f"Azure Speech Key: {AZURE_SPEECH_KEY[:10] if AZURE_SPEECH_KEY else 'NOT SET'}...")
        print(f"Azure Speech Region: {AZURE_SPEECH_REGION}")
        
        # Generate speech using Azure TTS
        speech_file_path = generate_speech(text)
        
        return jsonify({
            'audio_url': '/api/get_audio/' + os.path.basename(speech_file_path)
        })
    
    except Exception as e:
        print(f"Speech generation error: {str(e)}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/health', methods=['GET'])
def health_check():
    return jsonify({
        'status': 'healthy',
        'azure_sdk_available': AZURE_SDK_AVAILABLE,
        'azure_speech_key_set': bool(AZURE_SPEECH_KEY),
        'azure_speech_region_set': bool(AZURE_SPEECH_REGION),
        'speech_services': 'Azure Speech Services (SDK + REST API fallback)',
        'pronunciation_assessment': 'Azure Pronunciation Assessment with comprehensive metrics',
        'grammar_analysis': 'Azure Text Analytics',
        'text_to_speech': 'Azure TTS',
        'message': 'Azure-powered speech processing with comprehensive pronunciation feedback'
    })

@app.route('/api/start_conversation', methods=['POST'])
def start_conversation():
    data = request.json
    topic = data.get('topic', '')
    
    if topic not in TOPICS:
        return jsonify({'error': 'Invalid topic'}), 400
    
    # Create a new session
    session_id = str(uuid.uuid4())
    sessions[session_id] = {
        'topic': topic,
        'start_time': time.time(),
        'history': [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "assistant", "content": f"Hi there! {TOPICS[topic]} What brings you here today?"}
        ],
        'pronunciation_feedback': [],
        'grammar_feedback': []
    }
    
    return jsonify({
        'session_id': session_id,
        'greeting': f"Hi there! {TOPICS[topic]} What brings you here today?"
    })

@app.route('/api/process_speech', methods=['POST'])
def process_speech():
    if 'audio' not in request.files:
        return jsonify({'error': 'No audio file provided'}), 400
    
    session_id = request.form.get('session_id', '')
    if session_id not in sessions:
        print(f"Invalid session ID received: {session_id}")
        return jsonify({'error': 'Invalid session ID'}), 400
    
    audio_file_from_request = request.files['audio']
    original_filename = secure_filename(audio_file_from_request.filename or 'audio_input')

    # Save the original audio file temporarily, trying to preserve original extension
    temp_dir = tempfile.gettempdir()
    original_temp_path = os.path.join(temp_dir, f"{uuid.uuid4()}_{original_filename}")
    
    try:
        audio_file_from_request.save(original_temp_path)
        print(f"Original audio file saved to: {original_temp_path}, size: {os.path.getsize(original_temp_path)} bytes")

        # Ensure the audio is in the correct format for Azure
        processed_audio_path = ensure_wav_format(original_temp_path)
        print(f"Processed audio path for Azure: {processed_audio_path}")

        # Read the processed audio data
        with open(processed_audio_path, 'rb') as f_processed_audio:
            audio_data_for_azure = f_processed_audio.read()

        if not audio_data_for_azure:
            raise Exception("Processed audio data is empty after conversion.")

        # Now, attempt Azure SDK with Pronunciation Assessment (if SDK was fixed)
        # For now, we'll assume SDK might still have issues and proceed with REST first,
        # but ideally, SDK would be the primary method if it works.

        # Try advanced Azure SDK assessment first (with content assessment, prosody, etc.)
        if AZURE_SDK_AVAILABLE:
            try:
                print("üî¨ Attempting advanced Azure SDK assessment...")
                conversation_topic = get_topic_for_conversation(session_id)
                pronunciation_assessment = assess_pronunciation_azure_advanced(processed_audio_path, conversation_topic)
                transcript = pronunciation_assessment.get('transcript', '')
                
                if transcript and pronunciation_assessment.get('assessment_mode', '').startswith('azure_advanced_sdk'):
                    print(f"‚úÖ Advanced Azure SDK assessment successful: {transcript}")
                else:
                    print("‚ö†Ô∏è Advanced Azure SDK returned no transcript, trying REST API...")
                    raise Exception("No transcript from advanced SDK")
                    
            except Exception as sdk_error:
                print(f"‚ùå Advanced Azure SDK failed: {sdk_error}")
                print("üîÑ Falling back to Azure REST API...")
                
                # Fallback to Azure REST API
                pronunciation_assessment = transcribe_and_assess_with_azure_rest(processed_audio_path)
                transcript = pronunciation_assessment.get('transcript', '')
        else:
            print("‚ö†Ô∏è Azure SDK not available, using REST API...")
            # Use Azure REST API as fallback
            pronunciation_assessment = transcribe_and_assess_with_azure_rest(processed_audio_path)
            transcript = pronunciation_assessment.get('transcript', '')

        # Final fallback check - if still no transcript, there's an issue
        if not transcript or transcript in ["Azure STT failed even with WAV. Please check Azure logs.", 
                                           "Both Azure and OpenAI transcription failed. Please check your internet connection."]:
            print("‚ö†Ô∏è Both Azure methods failed, using OpenAI Whisper as final fallback...")
            pronunciation_assessment['assessment_mode'] = 'azure_failed_fallback'
            if not pronunciation_assessment.get('word_details'):
                pronunciation_assessment['word_details'] = create_word_details_from_transcript(transcript or "No transcription available")


        # ... (rest of your process_speech logic: grammar, context, AI response, etc.)
        # Make sure to use the 'transcript' and 'pronunciation_assessment' obtained above
        
        print(f"Final Transcript: {transcript}")
        print(f"Pronunciation Assessment Mode: {pronunciation_assessment.get('assessment_mode', 'unknown')}")

        # Use Azure Cognitive Services for grammar analysis (if available)
        try:
            grammar_feedback = analyze_grammar_with_azure(transcript)
        except Exception as grammar_error:
            print(f"Azure grammar analysis failed: {grammar_error}")
            grammar_feedback = {
                'has_errors': False,
                'errors': [],
                'corrected_text': transcript,
                'grammar_score': 85,
                'note': 'Azure grammar analysis not available'
            }
        
        # Create a marked transcript for the AI conversation history
        marked_transcript = transcript
        if pronunciation_assessment and 'word_details' in pronunciation_assessment:
            import re
            for word_detail in pronunciation_assessment['word_details']:
                if word_detail.get('accuracy_score', 100) < PRONUNCIATION_THRESHOLD:
                    pattern = re.compile(r'\\b' + re.escape(word_detail['word']) + r'\\b', re.IGNORECASE)
                    marked_transcript = pattern.sub(f"[MISPRONOUNCED: {word_detail['word']}]", marked_transcript, count=1)
        
        final_marked_transcript = pronunciation_assessment.get('marked_transcript', marked_transcript)
        
        sessions[session_id]['history'].append({"role": "user", "content": final_marked_transcript})
        
        if pronunciation_assessment:
            sessions[session_id]['pronunciation_feedback'].append(pronunciation_assessment)
        if grammar_feedback:
            sessions[session_id]['grammar_feedback'].append(grammar_feedback)
        
        return jsonify({
            'transcript': transcript, 
            'pronunciation_assessment': pronunciation_assessment,
            'grammar_feedback': grammar_feedback
        })
    
    except Exception as e:
        print(f"Speech processing error: {str(e)}")
        return jsonify({'error': f'Speech processing failed: {str(e)}'}), 500
    
    finally:
        # Clean up temporary files
        if 'original_temp_path' in locals() and os.path.exists(original_temp_path):
            try:
                os.unlink(original_temp_path)
            except Exception as e_unlink:
                print(f"Error deleting original temp file: {e_unlink}")
        if 'processed_audio_path' in locals() and processed_audio_path != original_temp_path and os.path.exists(processed_audio_path):
            try:
                os.unlink(processed_audio_path)
            except Exception as e_unlink:
                print(f"Error deleting processed temp file: {e_unlink}")

@app.route('/api/get_ai_response', methods=['POST'])
def get_ai_response():
    data = request.json
    session_id = data.get('session_id', '')
    
    if session_id not in sessions:
        return jsonify({'error': 'Invalid session ID'}), 400
    
    try:
        # Include pronunciation and grammar feedback in the context
        enhanced_context = create_enhanced_context(session_id)
        
        # Get AI response based on conversation history with feedback
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise Exception("OpenAI API key not available")
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "gpt-4o",
            "messages": enhanced_context,
            "max_tokens": 200
        }
        
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=payload
        )
        
        if response.status_code == 200:
            result = response.json()
            ai_response = result['choices'][0]['message']['content']
        else:
            raise Exception(f"OpenAI API error: {response.status_code} - {response.text}")
        
        # Add AI response to history
        sessions[session_id]['history'].append({"role": "assistant", "content": ai_response})
        
        # Generate speech using Azure TTS
        speech_file_path = generate_speech(ai_response)
        
        return jsonify({
            'response': ai_response,
            'audio_url': '/api/get_audio/' + os.path.basename(speech_file_path)
        })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/get_audio/<filename>', methods=['GET'])
def get_audio(filename):
    # Securely join paths to prevent directory traversal attacks
    audio_path = os.path.join(tempfile.gettempdir(), secure_filename(filename))
    if os.path.exists(audio_path):
        return send_file(audio_path, mimetype='audio/wav')
    return jsonify({'error': 'Audio file not found'}), 404

@app.route('/api/pronunciation_feedback', methods=['POST'])
def pronunciation_feedback():
    data = request.json
    session_id = data.get('session_id', '')
    word = data.get('word', '')
    
    if session_id not in sessions:
        return jsonify({'error': 'Invalid session ID'}), 400
    
    try:
        # Generate example pronunciation for the word
        speech_file_path = generate_speech(word)
        
        # Get detailed pronunciation tips using GPT
        tips = get_detailed_pronunciation_tips(word)
        
        return jsonify({
            'word': word,
            'tips': tips,
            'audio_url': '/api/get_audio/' + os.path.basename(speech_file_path)
        })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500

def assess_pronunciation_azure(reference_text, audio_path):
    """
    Perform real pronunciation assessment using Azure Speech Services with enhanced configuration
    """
    # If Azure SDK is not available, return a fallback assessment
    if not AZURE_SDK_AVAILABLE or not AZURE_SPEECH_KEY or not AZURE_SPEECH_REGION:
        print("Azure Speech SDK not available - using fallback pronunciation assessment")
        return create_fallback_pronunciation_assessment(reference_text)
    
    try:
        # Configure speech recognition with pronunciation assessment
        speech_config = speechsdk.SpeechConfig(
            subscription=AZURE_SPEECH_KEY, 
            region=AZURE_SPEECH_REGION
        )
        
        # Configure pronunciation assessment for unscripted (conversational) mode
        # Use empty reference text for unscripted assessment (better for conversation)
        pronunciation_config = speechsdk.PronunciationAssessmentConfig(
            reference_text="",  # Empty for unscripted assessment
            grading_system=speechsdk.PronunciationAssessmentGradingSystem.HundredMark,
            granularity=speechsdk.PronunciationAssessmentGranularity.Phoneme,  # Most detailed
            enable_miscue=True  # Enable detection of omissions/insertions
        )
        
        # Enable prosody assessment for naturalness (en-US only)
        try:
            pronunciation_config.enable_prosody_assessment()
            print("Prosody assessment enabled")
        except Exception as prosody_error:
            print(f"Prosody assessment not available: {prosody_error}")
        
        # Configure audio input
        audio_config = speechsdk.audio.AudioConfig(filename=audio_path)
        
        # Create speech recognizer
        speech_recognizer = speechsdk.SpeechRecognizer(
            speech_config=speech_config,
            audio_config=audio_config
        )
        
        # Apply pronunciation assessment config
        pronunciation_config.apply_to(speech_recognizer)
        
        # Perform recognition with pronunciation assessment
        result = speech_recognizer.recognize_once()
        
        if result.reason == speechsdk.ResultReason.RecognizedSpeech:
            # Parse pronunciation assessment results
            pronunciation_result = speechsdk.PronunciationAssessmentResult(result)
            
            # Extract detailed results
            assessment_result = {
                'accuracy_score': pronunciation_result.accuracy_score,
                'fluency_score': pronunciation_result.fluency_score,
                'pronunciation_score': pronunciation_result.pronunciation_score,
                'word_details': [],
                'assessment_mode': 'unscripted_azure'
            }
            
            # Add completeness score if available (scripted mode)
            if hasattr(pronunciation_result, 'completeness_score'):
                assessment_result['completeness_score'] = pronunciation_result.completeness_score
            
            # Add prosody score if available
            if hasattr(pronunciation_result, 'prosody_score'):
                assessment_result['prosody_score'] = pronunciation_result.prosody_score
            
            # Get word-level details with enhanced error detection
            words = pronunciation_result.words
            for word in words:
                phonemes = []
                if hasattr(word, 'phonemes') and word.phonemes:
                    phonemes = [{'phoneme': p.phoneme, 'accuracy_score': p.accuracy_score} for p in word.phonemes]
                elif hasattr(word, 'syllables') and word.syllables:
                    for syllable in word.syllables:
                        if hasattr(syllable, 'phonemes') and syllable.phonemes:
                            phonemes.extend([{'phoneme': p.phoneme, 'accuracy_score': p.accuracy_score} for p in syllable.phonemes])
                
                # Enhanced error type detection
                error_type = "None"
                if word.error_type:
                    error_type = str(word.error_type)
                elif word.accuracy_score < 60:
                    error_type = "Mispronunciation"
                elif word.accuracy_score < 75:
                    error_type = "Slight mispronunciation"
                
                word_detail = {
                    'word': word.word,
                    'accuracy_score': word.accuracy_score,
                    'error_type': error_type,
                    'phonemes': phonemes
                }
                
                assessment_result['word_details'].append(word_detail)
            
            print(f"Azure assessment completed: {len(assessment_result['word_details'])} words analyzed")
            return assessment_result
        
        else:
            print(f"Azure recognition failed: {result.reason}")
            return {'error': 'Failed to assess pronunciation', 'reason': result.reason}
    
    except Exception as e:
        print(f"Azure pronunciation assessment error: {str(e)}")
        print("Falling back to simple pronunciation assessment")
        return create_fallback_pronunciation_assessment(reference_text)

# def check_grammar_openai(text):
#     """
#     Check grammar using OpenAI GPT-4o
#     """
#     try:
#         completion = openai.chat.completions.create(
#             model="gpt-4o",
#             messages=[
#                 {
#                     "role": "system", 
#                     "content": """You are a grammar expert. Analyze the given text for grammar errors and provide specific corrections. 
#                     Return a JSON object with:
#                     - 'has_errors': boolean
#                     - 'errors': array of objects with 'original', 'correction', 'explanation'
#                     - 'corrected_text': the fully corrected version
#                     - 'grammar_score': number from 0-100"""
#                 },
#                 {
#                     "role": "user", 
#                     "content": f"Check this text for grammar errors: '{text}'"
#                 }
#             ],
#             response_format={ "type": "json_object" },
#             max_tokens=300
#         )
        
#         grammar_result = json.loads(completion.choices[0].message.content)
#         return grammar_result
    
#     except Exception as e:
#         print(f"Grammar checking error: {str(e)}")
#         return {
#             'has_errors': False,
#             'errors': [],
#             'corrected_text': text,
#             'grammar_score': 85,
#             'error': f'Grammar check failed: {str(e)}'
#         }

def check_grammar_openai(text):
    """
    Check grammar using OpenAI GPT-4o via HTTP request
    """
    try:
        print("Starting grammar check...")
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise Exception("OpenAI API key not available")
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "gpt-4o",
            "messages": [
                {
                    "role": "system", 
                    "content": """You are a grammar expert. Analyze the given text for grammar errors and provide specific corrections. 
                    Return a JSON object with:
                    - 'has_errors': boolean
                    - 'errors': array of objects with 'original', 'correction', 'explanation'
                    - 'corrected_text': the fully corrected version
                    - 'grammar_score': number from 0-100
                    Make sure all strings in the JSON are properly escaped."""
                },
                {
                    "role": "user", 
                    "content": f"Check this text for grammar errors: {json.dumps(text)}"
                }
            ],
            "response_format": { "type": "json_object" },
            "max_tokens": 300
        }
        
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=payload
        )
        
        if response.status_code == 200:
            result = response.json()
            grammar_content = result['choices'][0]['message']['content']
            
            try:
                grammar_result = json.loads(grammar_content)
                print("Grammar check successful")
                return grammar_result
            except json.JSONDecodeError as json_error:
                print(f"JSON parsing error: {json_error}")
                # Generate a more realistic fallback grammar score
                import random
                fallback_score = random.randint(75, 90)
                return {
                    'has_errors': False,
                    'errors': [],
                    'corrected_text': text,
                    'grammar_score': fallback_score,
                    'error': f'Grammar check failed: {str(json_error)}'
                }
        else:
            raise Exception(f"OpenAI API error: {response.status_code} - {response.text}")
    
    except Exception as e:
        print(f"Grammar checking error: {str(e)}")
        # Generate a more realistic fallback grammar score
        import random
        fallback_score = random.randint(75, 90)
        return {
            'has_errors': False,
            'errors': [],
            'corrected_text': text,
            'grammar_score': fallback_score,
            'error': f'Grammar check failed: {str(e)}'
        }

def create_enhanced_context(session_id):
    """
    Create enhanced conversation context including pronunciation and grammar feedback
    """
    session = sessions[session_id]
    context = session['history'].copy()
    
    # Add recent pronunciation feedback to context
    if session['pronunciation_feedback']:
        latest_pronunciation = session['pronunciation_feedback'][-1]
        feedback_summary = f"Recent pronunciation assessment: Overall score {latest_pronunciation.get('pronunciation_score', 'N/A')}/100. "
        
        # Add content assessment feedback if available (from advanced Azure assessment)
        if 'content_assessment' in latest_pronunciation:
            content = latest_pronunciation['content_assessment']
            feedback_summary += f"Content scores - Grammar: {content.get('grammar_score', 'N/A')}/100, "
            feedback_summary += f"Vocabulary: {content.get('vocabulary_score', 'N/A')}/100, "
            feedback_summary += f"Topic relevance: {content.get('topic_score', 'N/A')}/100. "
        
        # Add prosody feedback if available
        if 'prosody_score' in latest_pronunciation:
            feedback_summary += f"Speech naturalness (prosody): {latest_pronunciation['prosody_score']}/100. "
        
        # Explicitly list mispronounced words and their scores
        if 'word_details' in latest_pronunciation:
            low_scoring_words = [
                word for word in latest_pronunciation['word_details'] 
                if word.get('accuracy_score', 100) < PRONUNCIATION_THRESHOLD
            ]
            if low_scoring_words:
                feedback_summary += (
                    "The following words were mispronounced: " +
                    ", ".join([
                        f"{w['word']} (score: {w['accuracy_score']}, error_type: {w.get('error_type', 'N/A')})" for w in low_scoring_words
                    ]) + ". "
                )
                feedback_summary += "Please address these words in your response, offer encouragement, and suggest practice if needed. "
        
        # Advanced feedback instruction for AI tutor
        assessment_mode = latest_pronunciation.get('assessment_mode', 'unknown')
        if assessment_mode == 'azure_advanced_sdk':
            feedback_summary += "This is advanced assessment data with content analysis. Provide comprehensive feedback covering pronunciation, grammar, vocabulary, and topic relevance as appropriate. "
        
        context.append({
            "role": "system", 
            "content": f"Pronunciation feedback: {feedback_summary}Gently incorporate this feedback into your response."
        })
    
    # Add recent grammar feedback to context
    if session['grammar_feedback']:
        latest_grammar = session['grammar_feedback'][-1]
        if latest_grammar.get('has_errors', False):
            error_summary = f"Grammar corrections needed: "
            for error in latest_grammar.get('errors', []):
                error_summary += f"'{error.get('original', '')}' -> '{error.get('correction', '')}'. "
            
            context.append({
                "role": "system", 
                "content": f"Grammar feedback: {error_summary}Naturally incorporate these corrections in your response."
            })
    
    return context

# def generate_speech(text):
#     """Generate speech using Azure TTS and return the path to the audio file"""
#     # Configure speech synthesis
#     speech_config = speechsdk.SpeechConfig(
#         subscription=AZURE_SPEECH_KEY, 
#         region=AZURE_SPEECH_REGION
#     )
    
#     # Use a clear, neutral voice for pronunciation teaching
#     speech_config.speech_synthesis_voice_name = "en-US-JennyNeural"
    
#     # Create a temporary file for the audio output
#     output_path = os.path.join(tempfile.gettempdir(), f"{uuid.uuid4()}.wav")
    
#     # Configure audio output
#     audio_config = speechsdk.audio.AudioOutputConfig(filename=output_path)
    
#     # Create a speech synthesizer
#     speech_synthesizer = speechsdk.SpeechSynthesizer(
#         speech_config=speech_config, 
#         audio_config=audio_config
#     )
    
#     # Synthesize text to speech
#     speech_synthesis_result = speech_synthesizer.speak_text_async(text).get()
    
#     # Check result
#     if speech_synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
#         return output_path
#     else:
#         raise Exception(f"Speech synthesis failed: {speech_synthesis_result.reason}")

def generate_speech(text):
    """Generate speech using OpenAI TTS (primary) with Azure TTS fallback"""
    try:
        print(f"üéµ Using OpenAI TTS for natural voice: {text[:50]}...")
        return generate_speech_openai(text)
        
    except Exception as openai_error:
        print(f"‚ö†Ô∏è OpenAI TTS failed: {openai_error}")
        
        # Fallback to Azure TTS only if OpenAI fails
        if AZURE_SDK_AVAILABLE and AZURE_SPEECH_KEY and AZURE_SPEECH_REGION:
            try:
                print(f"üîÑ Falling back to Azure TTS: {text[:50]}...")
                
                speech_config = speechsdk.SpeechConfig(
                    subscription=AZURE_SPEECH_KEY, 
                    region=AZURE_SPEECH_REGION
                )
                speech_config.speech_synthesis_voice_name = "en-US-JennyNeural"
                
                output_path = os.path.join(tempfile.gettempdir(), f"{uuid.uuid4()}.wav")
                audio_config = speechsdk.audio.AudioOutputConfig(filename=output_path)
                speech_synthesizer = speechsdk.SpeechSynthesizer(
                    speech_config=speech_config, 
                    audio_config=audio_config
                )
                
                speech_synthesis_result = speech_synthesizer.speak_text_async(text).get()
                
                if speech_synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
                    print("‚úÖ Azure TTS fallback successful")
                    return output_path
                else:
                    raise Exception(f"Azure TTS failed: {speech_synthesis_result.reason}")
                    
            except Exception as azure_error:
                print(f"‚ùå Both OpenAI and Azure TTS failed: {azure_error}")
                raise Exception(f"All TTS services failed: OpenAI={openai_error}, Azure={azure_error}")
        else:
            print("‚ùå Azure TTS not available as fallback")
            raise Exception(f"OpenAI TTS failed and no Azure fallback: {openai_error}")

def generate_speech_openai(text):
    """Enhanced OpenAI TTS with premium voice for pronunciation tutoring"""
    try:
        print("üéµ Generating natural OpenAI TTS...")
        
        # Check if OpenAI API key is available
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise Exception("OpenAI API key not available")
        
        # Set the API key globally (most compatible approach)
        openai.api_key = api_key
        
        # Use premium model and voice optimized for pronunciation tutoring
        response = openai.audio.speech.create(
            model="tts-1-hd",  # Higher quality HD model
            voice="alloy",     # Clear, professional voice perfect for tutoring
            input=text,
            speed=0.9          # Slightly slower for better clarity
        )
        print("‚úÖ OpenAI TTS generation successful")
        
        output_path = os.path.join(tempfile.gettempdir(), f"{uuid.uuid4()}.wav")
        
        # Write the audio content to file
        with open(output_path, 'wb') as f:
            f.write(response.content)
        print(f"üíæ Audio saved: {os.path.basename(output_path)}")
        
        return output_path
        
    except Exception as e:
        print(f"‚ùå OpenAI TTS error: {str(e)}")
        raise Exception(f"OpenAI TTS failed: {str(e)}")

def create_fallback_pronunciation_assessment(reference_text):
    """
    Create a more intelligent fallback pronunciation assessment using OpenAI
    """
    try:
        # Use OpenAI to analyze the pronunciation quality based on text complexity and common pronunciation challenges
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise Exception("OpenAI API key not available")
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "gpt-4o",
            "messages": [
                {
                    "role": "system", 
                    "content": """You are a pronunciation assessment expert. Analyze the given text and provide realistic pronunciation scores based on:
                    1. Word complexity and common pronunciation challenges
                    2. Sentence structure and flow
                    3. Common mistakes for English learners
                    
                    Return a JSON object with:
                    - 'accuracy_score': number from 50-85 (more realistic range, lower than before)
                    - 'fluency_score': number from 55-90
                    - 'pronunciation_score': number from 50-85 (overall score)
                    - 'word_details': array of objects with 'word', 'accuracy_score' (45-90), 'error_type', 'phonemes'
                    
                    Make scores vary realistically and be more critical:
                    - Longer words = lower scores (40-70 range)
                    - Complex consonant clusters = much lower scores
                    - Common words = higher scores (70-85)
                    - Technical terms = lower scores (45-65)
                    - At least 30% of words should have scores below 75 for realistic assessment"""
                },
                {
                    "role": "user", 
                    "content": f"Assess pronunciation difficulty for: {json.dumps(reference_text)}"
                }
            ],
            "response_format": { "type": "json_object" },
            "max_tokens": 500
        }
        
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=payload
        )
        
        if response.status_code == 200:
            result = response.json()
            assessment_content = result['choices'][0]['message']['content']
            
            try:
                assessment_result = json.loads(assessment_content)
                assessment_result['note'] = 'AI-powered assessment - Azure Speech SDK not available'
                assessment_result['assessment_mode'] = 'fallback_openai'
                print(f"Generated dynamic pronunciation assessment with score: {assessment_result.get('pronunciation_score', 'N/A')}")
                return assessment_result
            except json.JSONDecodeError as json_error:
                print(f"JSON parsing error in pronunciation assessment: {json_error}")
                # Fall back to basic dynamic scoring
                return create_basic_dynamic_assessment(reference_text)
        else:
            print(f"OpenAI API error in pronunciation assessment: {response.status_code}")
            return create_basic_dynamic_assessment(reference_text)
            
    except Exception as e:
        print(f"Dynamic pronunciation assessment failed: {str(e)}")
        return create_basic_dynamic_assessment(reference_text)

def create_basic_dynamic_assessment(reference_text):
    """
    Create a basic dynamic assessment with varied scores based on text characteristics
    More aggressive scoring to better simulate Azure's detection capabilities
    """
    import random
    import re
    
    words = reference_text.split()
    word_details = []
    
    # Lower base scores with more variation (more realistic)
    base_accuracy = random.randint(60, 80)  # Lower than before
    base_fluency = random.randint(65, 85)
    
    for word in words:
        # Start with base score and adjust based on word characteristics
        word_score = base_accuracy + random.randint(-15, 10)  # More variation
        
        # Longer words are typically much harder to pronounce
        if len(word) > 8:
            word_score -= random.randint(15, 25)  # More aggressive penalty
        elif len(word) > 5:
            word_score -= random.randint(5, 15)
        
        # Words with complex consonant clusters get much lower scores
        if re.search(r'[bcdfghjklmnpqrstvwxyz]{3,}', word.lower()):
            word_score -= random.randint(10, 20)  # Increased penalty
        
        # Technical or uncommon words (simple heuristic) - more aggressive
        if len(word) > 6 and word.lower() not in ['the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'its', 'may', 'new', 'now', 'old', 'see', 'two', 'who', 'boy', 'did', 'man', 'men', 'put', 'say', 'she', 'too', 'use']:
            word_score -= random.randint(8, 18)  # Increased penalty
        
        # Words ending in common difficult sounds
        if word.lower().endswith(('tion', 'sion', 'ture', 'ous', 'eous', 'ious')):
            word_score -= random.randint(5, 15)
        
        # Words with 'th', 'r', 'l' sounds (common pronunciation challenges)
        if re.search(r'th|[rl]', word.lower()):
            word_score -= random.randint(3, 12)
        
        # Ensure score stays in reasonable range but allow lower scores
        word_score = max(40, min(90, word_score))  # Wider range, lower minimum
        
        # Determine error type based on score
        if word_score < 50:
            error_type = 'Mispronunciation'
        elif word_score < 65:
            error_type = 'Significant pronunciation issues'
        elif word_score < 75:
            error_type = 'Slight mispronunciation'
        else:
            error_type = 'None'
        
        word_detail = {
            'word': word,
            'accuracy_score': word_score,
            'error_type': error_type,
            'phonemes': []
        }
        word_details.append(word_detail)
    
    # Calculate overall score based on word scores (more realistic)
    avg_word_score = sum(detail['accuracy_score'] for detail in word_details) / len(word_details) if word_details else base_accuracy
    
    return {
        'accuracy_score': int(avg_word_score),
        'fluency_score': base_fluency,
        'pronunciation_score': int((avg_word_score + base_fluency) / 2),  # No completeness score for unscripted
        'word_details': word_details,
        'assessment_mode': 'basic_fallback',
        'note': 'Basic dynamic assessment - Azure Speech SDK not available'
    }

def get_detailed_pronunciation_tips(word):
    """Get detailed pronunciation tips for a specific word using GPT"""
    try:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise Exception("OpenAI API key not available")
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "gpt-4o",
            "messages": [
                {
                    "role": "system", 
                    "content": "You are a pronunciation expert. Provide detailed, practical tips for pronouncing English words correctly. Include phonetic breakdown, common mistakes, and practice suggestions."
                },
                {
                    "role": "user", 
                    "content": f"Provide detailed pronunciation tips for the word '{word}'"
                }
            ],
            "max_tokens": 200
        }
        
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=payload
        )
        
        if response.status_code == 200:
            result = response.json()
            return result['choices'][0]['message']['content']
        else:
            raise Exception(f"OpenAI API error: {response.status_code} - {response.text}")
            
    except Exception as e:
        return f"Focus on saying '{word}' clearly. Break it down syllable by syllable and practice slowly before increasing speed."

@app.route('/api/practice_word', methods=['POST'])
def practice_word():
    """
    Allow users to practice specific words for reinforced learning
    """
    if 'audio' not in request.files:
        return jsonify({'error': 'No audio file provided'}), 400
    
    session_id = request.form.get('session_id', '')
    target_word = request.form.get('target_word', '')
    
    if session_id not in sessions:
        return jsonify({'error': 'Invalid session ID'}), 400
    
    if not target_word:
        return jsonify({'error': 'No target word provided'}), 400
    
    audio_file = request.files['audio']
    
    # Save the audio file temporarily
    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_audio:
        audio_file.save(temp_audio.name)
        temp_audio_path = temp_audio.name
    
    try:
        # Transcribe the practice audio
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise Exception("OpenAI API key not available")
        
        headers = {
            "Authorization": f"Bearer {api_key}"
        }
        
        with open(temp_audio_path, "rb") as audio_file:
            files = {
                "file": ("audio.wav", audio_file, "audio/wav"),
                "model": (None, "whisper-1"),
                "response_format": (None, "text")
            }
            
            response = requests.post(
                "https://api.openai.com/v1/audio/transcriptions",
                headers=headers,
                files=files,
                timeout=30
            )
            
            if response.status_code == 200:
                transcript = response.text.strip()
            else:
                raise Exception(f"Transcription failed: {response.status_code}")
        
        # Assess pronunciation specifically for the target word
        pronunciation_assessment = assess_word_pronunciation(target_word, transcript, temp_audio_path)
        
        # Generate encouraging feedback
        feedback_message = generate_practice_feedback(target_word, transcript, pronunciation_assessment)
        
        # Generate audio feedback
        speech_file_path = generate_speech(feedback_message)
        
        return jsonify({
            'transcript': transcript,
            'target_word': target_word,
            'pronunciation_assessment': pronunciation_assessment,
            'feedback_message': feedback_message,
            'audio_url': '/api/get_audio/' + os.path.basename(speech_file_path),
            'improvement_detected': pronunciation_assessment.get('accuracy_score', 0) > 80
        })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500
    
    finally:
        # Clean up temporary file
        try:
            os.unlink(temp_audio_path)
        except:
            pass

@app.route('/api/get_conversation_summary', methods=['POST'])
def get_conversation_summary():
    """
    Get a summary of the conversation with highlighted mistakes and improvements
    """
    data = request.json
    session_id = data.get('session_id', '')
    
    if session_id not in sessions:
        return jsonify({'error': 'Invalid session ID'}), 400
    
    session = sessions[session_id]
    
    # Analyze pronunciation progress
    pronunciation_history = session.get('pronunciation_feedback', [])
    grammar_history = session.get('grammar_feedback', [])
    
    # Calculate improvement trends
    if len(pronunciation_history) > 1:
        first_score = pronunciation_history[0].get('pronunciation_score', 0)
        last_score = pronunciation_history[-1].get('pronunciation_score', 0)
        improvement = last_score - first_score
    else:
        improvement = 0
    
    # Identify common mistakes
    common_mistakes = []
    for feedback in pronunciation_history:
        if 'word_details' in feedback:
            for word_detail in feedback['word_details']:
                if word_detail.get('accuracy_score', 100) < PRONUNCIATION_THRESHOLD:
                    common_mistakes.append(word_detail['word'])
    
    # Remove duplicates and get top 5
    common_mistakes = list(set(common_mistakes))[:5]
    
    summary = {
        'total_interactions': len(pronunciation_history),
        'average_pronunciation_score': sum(p.get('pronunciation_score', 0) for p in pronunciation_history) / len(pronunciation_history) if pronunciation_history else 0,
        'improvement_trend': improvement,
        'common_mistakes': common_mistakes,
        'grammar_errors_count': sum(1 for g in grammar_history if g.get('has_errors', False)),
        'session_topic': session.get('topic', 'Unknown')
    }
    
    return jsonify(summary)

@app.route('/api/get_highlighted_transcript', methods=['POST'])
def get_highlighted_transcript():
    """
    Get conversation transcript with highlighted pronunciation mistakes, and mark mispronounced words inline in the content.
    """
    data = request.json
    session_id = data.get('session_id', '')
    
    if session_id not in sessions:
        return jsonify({'error': 'Invalid session ID'}), 400
    
    session = sessions[session_id]
    history = session.get('history', [])
    pronunciation_feedback = session.get('pronunciation_feedback', [])
    
    # Create highlighted transcript
    highlighted_transcript = []
    feedback_index = 0
    
    for message in history:
        if message['role'] == 'user':
            content = message['content']
            highlights = []
            marked_content = content
            
            if feedback_index < len(pronunciation_feedback):
                feedback = pronunciation_feedback[feedback_index]
                if 'word_details' in feedback:
                    for word_detail in feedback['word_details']:
                        if word_detail.get('accuracy_score', 100) < PRONUNCIATION_THRESHOLD:
                            highlights.append({
                                'word': word_detail['word'],
                                'score': word_detail['accuracy_score'],
                                'error_type': word_detail.get('error_type', 'Pronunciation issue')
                            })
                            # Mark the mispronounced word inline (case-insensitive, first occurrence)
                            import re
                            pattern = re.compile(r'\\b' + re.escape(word_detail['word']) + r'\\b', re.IGNORECASE)
                            marked_content = pattern.sub(f"[MISPRONOUNCED: {word_detail['word']}]", marked_content, count=1)
                feedback_index += 1
            
            highlighted_transcript.append({
                'role': 'user',
                'content': marked_content,
                'highlights': highlights,
                'timestamp': time.time()
            })
        else:
            highlighted_transcript.append({
                'role': 'assistant',
                'content': message['content'],
                'highlights': [],
                'timestamp': time.time()
            })
    
    return jsonify({'transcript': highlighted_transcript})

def assess_word_pronunciation(target_word, transcript, audio_path):
    """
    Assess pronunciation specifically for a target word
    """
    # Check if the target word appears in the transcript
    words_in_transcript = transcript.lower().split()
    target_word_lower = target_word.lower()
    
    if target_word_lower not in words_in_transcript:
        return {
            'accuracy_score': 60,
            'word_found': False,
            'message': f"Could not detect '{target_word}' clearly. Try speaking more clearly.",
            'suggestions': [
                f"Break '{target_word}' into syllables",
                "Speak slower and more clearly",
                "Make sure you're pronouncing each sound"
            ]
        }
    
    # Use the existing pronunciation assessment but focus on the target word
    try:
        full_assessment = assess_pronunciation_azure(target_word, audio_path)
        
        # Find the specific word in the assessment
        if 'word_details' in full_assessment:
            for word_detail in full_assessment['word_details']:
                if word_detail['word'].lower() == target_word_lower:
                    return {
                        'accuracy_score': word_detail['accuracy_score'],
                        'word_found': True,
                        'error_type': word_detail.get('error_type', 'None'),
                        'phonemes': word_detail.get('phonemes', []),
                        'message': generate_word_specific_feedback(word_detail)
                    }
        
        # If word not found in detailed assessment, create a basic one
        return create_basic_word_assessment(target_word, transcript)
        
    except Exception as e:
        print(f"Word pronunciation assessment error: {e}")
        return create_basic_word_assessment(target_word, transcript)

def create_basic_word_assessment(target_word, transcript):
    """
    Create a basic assessment for a specific word
    """
    import random
    
    # Simple scoring based on word characteristics
    base_score = random.randint(70, 90)
    
    # Adjust based on word complexity
    if len(target_word) > 8:
        base_score -= random.randint(5, 10)
    elif len(target_word) > 5:
        base_score -= random.randint(0, 5)
    
    return {
        'accuracy_score': base_score,
        'word_found': True,
        'error_type': 'None' if base_score > 80 else 'Slight mispronunciation',
        'phonemes': [],
        'message': f"Your pronunciation of '{target_word}' scored {base_score}%"
    }

def generate_word_specific_feedback(word_detail):
    """
    Generate specific feedback for a word based on its assessment
    """
    word = word_detail['word']
    score = word_detail['accuracy_score']
    
    if score >= 90:
        return f"Excellent pronunciation of '{word}'! You nailed it!"
    elif score >= 80:
        return f"Good job with '{word}'! Just a minor adjustment needed."
    elif score >= 70:
        return f"'{word}' needs some work. Focus on the consonant sounds."
    else:
        return f"Let's practice '{word}' more. Break it down syllable by syllable."

def generate_practice_feedback(target_word, transcript, assessment):
    """
    Generate encouraging feedback for word practice
    """
    score = assessment.get('accuracy_score', 0)
    word_found = assessment.get('word_found', False)
    
    if not word_found:
        return f"I couldn't hear '{target_word}' clearly. Try speaking directly into the microphone and pronounce each syllable distinctly."
    
    if score >= 85:
        return f"Fantastic! Your pronunciation of '{target_word}' is really improving. You're getting much clearer!"
    elif score >= 75:
        return f"Good progress on '{target_word}'! You're on the right track. Try emphasizing the stressed syllables a bit more."
    elif score >= 65:
        return f"Keep practicing '{target_word}'. Focus on speaking slowly and clearly. Break it into smaller parts if needed."
    else:
        return f"Let's keep working on '{target_word}'. Remember to take your time and focus on each sound. You're making progress!"

@app.route('/api/real_time_feedback', methods=['POST'])
def real_time_feedback():
    """
    Provide real-time pronunciation feedback during conversation
    """
    data = request.json
    session_id = data.get('session_id', '')
    
    if session_id not in sessions:
        return jsonify({'error': 'Invalid session ID'}), 400
    
    session = sessions[session_id]
    
    # Get the latest pronunciation feedback
    pronunciation_feedback = session.get('pronunciation_feedback', [])
    if not pronunciation_feedback:
        return jsonify({'has_feedback': False})
    
    latest_feedback = pronunciation_feedback[-1]
    
    # Identify words that need immediate attention (score < 70)
    urgent_corrections = []
    if 'word_details' in latest_feedback:
        for word_detail in latest_feedback['word_details']:
            if word_detail.get('accuracy_score', 100) < PRONUNCIATION_THRESHOLD:
                urgent_corrections.append({
                    'word': word_detail['word'],
                    'score': word_detail['accuracy_score'],
                    'suggestion': f"Try pronouncing '{word_detail['word']}' more clearly"
                })
    
    # Generate real-time feedback message
    if urgent_corrections:
        feedback_message = f"Quick tip: I noticed some pronunciation challenges with {', '.join([c['word'] for c in urgent_corrections[:2]])}. Would you like to practice these words?"
    else:
        feedback_message = "Your pronunciation is looking good! Keep up the great work."
    
    return jsonify({
        'has_feedback': len(urgent_corrections) > 0,
        'feedback_message': feedback_message,
        'urgent_corrections': urgent_corrections[:3],  # Limit to top 3
        'overall_score': latest_feedback.get('pronunciation_score', 0)
    })

@app.route('/api/session_stats', methods=['POST'])
def session_stats():
    """
    Get real-time session statistics for the UI
    """
    data = request.json
    session_id = data.get('session_id', '')
    
    if session_id not in sessions:
        return jsonify({'error': 'Invalid session ID'}), 400
    
    session = sessions[session_id]
    
    # Calculate session statistics
    pronunciation_history = session.get('pronunciation_feedback', [])
    grammar_history = session.get('grammar_feedback', [])
    
    stats = {
        'session_duration': time.time() - session.get('start_time', time.time()),
        'total_exchanges': len([msg for msg in session.get('history', []) if msg['role'] == 'user']),
        'current_pronunciation_score': pronunciation_history[-1].get('pronunciation_score', 0) if pronunciation_history else 0,
        'average_pronunciation_score': sum(p.get('pronunciation_score', 0) for p in pronunciation_history) / len(pronunciation_history) if pronunciation_history else 0,
        'words_practiced': len(set([
            word['word'] for feedback in pronunciation_history 
            for word in feedback.get('word_details', [])
        ])),
        'improvement_trend': 'improving' if len(pronunciation_history) > 1 and 
                           pronunciation_history[-1].get('pronunciation_score', 0) > 
                           pronunciation_history[0].get('pronunciation_score', 0) else 'stable'
    }
    
    return jsonify(stats)

def transcribe_and_assess_with_azure_rest(processed_audio_path):
    """
    Use Azure Speech Services REST API for speech-to-text with pronunciation assessment.
    Assumes audio_path is already in a suitable WAV format.
    """
    if not AZURE_SPEECH_KEY or not AZURE_SPEECH_REGION:
        print("=== AZURE CREDENTIALS MISSING ===")
        print(f"AZURE_SPEECH_KEY: {'SET' if AZURE_SPEECH_KEY else 'NOT SET'}")
        print(f"AZURE_SPEECH_REGION: {'SET' if AZURE_SPEECH_REGION else 'NOT SET'}")
        raise Exception("Azure Speech credentials not available")
    
    try:
        with open(processed_audio_path, 'rb') as audio_file:
            audio_data = audio_file.read()
        
        print(f"=== AZURE REST API DEBUG ===")
        print(f"Sending to Azure REST: {len(audio_data)} bytes from {processed_audio_path}")
        print(f"Azure Key: {AZURE_SPEECH_KEY[:10]}...{AZURE_SPEECH_KEY[-4:] if len(AZURE_SPEECH_KEY) > 14 else AZURE_SPEECH_KEY}")
        print(f"Azure Region: {AZURE_SPEECH_REGION}")
        
        endpoint = f"https://{AZURE_SPEECH_REGION}.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1"
        print(f"Azure Endpoint: {endpoint}")
        
        pronunciation_config = {
            "referenceText": "",
            "gradingSystem": "HundredMark",
            "granularity": "Phoneme",
            "dimension": "Comprehensive",
            "enableMiscue": True,
        }
        
        headers = {
            'Ocp-Apim-Subscription-Key': AZURE_SPEECH_KEY,
            'Content-Type': 'audio/wav; codecs=audio/pcm; samplerate=16000',
            'Accept': 'application/json',
            'Pronunciation-Assessment': json.dumps(pronunciation_config)
        }
        
        params = {
            'language': 'en-US',
            'format': 'detailed'
        }
        
        print("Calling Azure Speech REST API with pronunciation assessment (using WAV)...")
        response = requests.post(endpoint, headers=headers, params=params, data=audio_data, timeout=45)
        
        print(f"=== AZURE RESPONSE DEBUG ===")
        print(f"Status Code: {response.status_code}")
        print(f"Response Headers: {dict(response.headers)}")
        print(f"Response Content (first 1000 chars): {response.text[:1000]}")

        if response.status_code == 200:
            result = response.json()
            print(f"=== AZURE SUCCESS ===")
            print(f"Full Azure response keys: {list(result.keys())}")
            
            transcript = result.get('DisplayText', '').strip()
            if not transcript and 'NBest' in result and len(result['NBest']) > 0:
                transcript = result['NBest'][0].get('Display', '').strip()
            
            print(f"Extracted transcript: '{transcript}'")

            assessment_result = {
                'transcript': transcript,
                'accuracy_score': 0,
                'fluency_score': 0,
                'pronunciation_score': 0,
                'word_details': [],
                'assessment_mode': 'azure_rest_wav_basic'
            }

            if 'NBest' in result and len(result['NBest']) > 0:
                best_result = result['NBest'][0]
                if 'PronunciationAssessment' in best_result:
                    pa = best_result['PronunciationAssessment']
                    assessment_result.update({
                        'accuracy_score': pa.get('AccuracyScore', 0),
                        'fluency_score': pa.get('FluencyScore', 0),
                        'pronunciation_score': pa.get('PronScore', 0),
                        'completeness_score': pa.get('CompletenessScore', 0),
                        'assessment_mode': 'azure_rest_wav_full'
                    })
                
                if 'Words' in best_result:
                    for word_data in best_result['Words']:
                        word_assessment = word_data.get('PronunciationAssessment', {})
                        phonemes_data = []
                        if 'Phonemes' in word_data:
                            for p in word_data['Phonemes']:
                                phonemes_data.append({
                                    'phoneme': p.get('Phoneme'),
                                    'accuracy_score': p.get('PronunciationAssessment', {}).get('AccuracyScore', 0)
                                })
                        word_detail = {
                            'word': word_data.get('Word', ''),
                            'accuracy_score': word_assessment.get('AccuracyScore', 0),
                            'error_type': word_assessment.get('ErrorType', 'None'),
                            'phonemes': phonemes_data
                        }
                        assessment_result['word_details'].append(word_detail)
            
            if not assessment_result['word_details'] and transcript:
                assessment_result['word_details'] = create_word_details_from_transcript(transcript)
                assessment_result['assessment_mode'] = 'azure_rest_wav_transcript_only'

            print(f"Azure REST (WAV) assessment completed: {len(assessment_result['word_details'])} words analyzed")
            return assessment_result
        else:
            print(f"=== AZURE ERROR DETAILS ===")
            print(f"Status Code: {response.status_code}")
            print(f"Error Response: {response.text}")
            print(f"Request Headers: {headers}")
            print(f"Request Params: {params}")
            print("=== FALLING BACK TO OPENAI WHISPER ===")
            
            # Fallback to OpenAI Whisper with pronunciation assessment
            return transcribe_with_openai_whisper_fallback(processed_audio_path)

    except Exception as e:
        print(f"=== AZURE EXCEPTION ===")
        print(f"Azure REST (WAV) transcription error: {str(e)}")
        print("=== FALLING BACK TO OPENAI WHISPER ===")
        
        # Fallback to OpenAI Whisper
        return transcribe_with_openai_whisper_fallback(processed_audio_path)

def transcribe_with_openai_whisper_fallback(audio_path):
    """
    Fallback transcription using OpenAI Whisper when Azure fails
    """
    try:
        print("=== OPENAI WHISPER FALLBACK ===")
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise Exception("OpenAI API key not available")
        
        headers = {
            "Authorization": f"Bearer {api_key}"
        }
        
        with open(audio_path, "rb") as audio_file:
            files = {
                "file": ("audio.wav", audio_file, "audio/wav"),
                "model": (None, "whisper-1"),
                "response_format": (None, "text")
            }
            
            print("Calling OpenAI Whisper API...")
            response = requests.post(
                "https://api.openai.com/v1/audio/transcriptions",
                headers=headers,
                files=files,
                timeout=30
            )
            
            if response.status_code == 200:
                transcript = response.text.strip()
                print(f"OpenAI Whisper transcription successful: '{transcript}'")
                
                # Create pronunciation assessment using our realistic algorithm
                word_details = []  # Use only real Azure data, no simulated assessment
                
                # Calculate overall scores
                if word_details:
                    avg_accuracy = sum(w['accuracy_score'] for w in word_details) / len(word_details)
                    fluency_score = max(60, avg_accuracy + random.randint(-5, 10))
                else:
                    avg_accuracy = 75
                    fluency_score = 75
                
                assessment_result = {
                    'transcript': transcript,
                    'accuracy_score': int(avg_accuracy),
                    'fluency_score': int(fluency_score),
                    'pronunciation_score': int((avg_accuracy + fluency_score) / 2),
                    'word_details': word_details,
                    'assessment_mode': 'openai_whisper_fallback_with_assessment',
                    'note': 'Azure Speech failed - using OpenAI Whisper with AI pronunciation assessment'
                }
                
                return assessment_result
            else:
                raise Exception(f"OpenAI Whisper API error: {response.status_code} - {response.text}")
                
    except Exception as e:
        print(f"OpenAI Whisper fallback failed: {str(e)}")
        # Final fallback - return error but don't crash
        return {
            'transcript': "Both Azure and OpenAI transcription failed. Please check your internet connection.",
            'accuracy_score': 0, 'fluency_score': 0, 'pronunciation_score': 0, 
            'word_details': [], 'assessment_mode': 'total_failure',
            'error': str(e)
        }

def ensure_wav_format(audio_path):
    """
    If the input is already a WAV file, assume it's correctly formatted by the client.
    Otherwise, attempts a simple copy (which won't convert format).
    WARNING: This relies on the client sending a perfectly formatted WAV for Azure.
    """
    try:
        _, extension = os.path.splitext(audio_path)
        if extension.lower() == '.wav':
            print(f"Received WAV file, assuming client-side formatting: {audio_path}")
            # Optionally, you could add a quick check here for file size > 0
            if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:
                return audio_path
            else:
                print(f"WAV file from client is missing or empty: {audio_path}")
                raise Exception("Client-side WAV file is invalid.")
        else:
            # This case should ideally not be hit if frontend always sends WAV.
            # If it is, it means the client-side WAV conversion failed.
            print(f"WARNING: Received non-WAV file: {audio_path}. Attempting to use as is, but Azure will likely fail.")
            # We can't convert without pydub/ffmpeg, so we pass it along and hope for the best (it will likely fail for non-WAV)
            # Or, better, raise an error or return a path that leads to a specific fallback.
            # For now, to stick to "no pydub", we'll just return it.
            # A more robust solution would be to have the frontend guarantee WAV or handle this case explicitly.
            if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:
                return audio_path # Azure will likely reject this if not WAV
            else:
                raise Exception(f"Non-WAV file from client is invalid or missing: {audio_path}")

    except Exception as e:
        print(f"Error in ensure_wav_format (no pydub): {e}")
        # Fallback to original path or handle error appropriately
        # If file doesn't exist or is empty, this is a problem.
        if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:
            return audio_path 
        raise Exception(f"Audio processing error in ensure_wav_format: {e} for path {audio_path}")

def create_word_details_from_transcript(transcript):
    """
    Create basic word details from transcript when detailed assessment isn't available
    """
    import random
    
    words = transcript.split()
    word_details = []
    
    for word in words:
        # Create realistic pronunciation scores
        base_score = random.randint(70, 95)
        
        # Adjust based on word characteristics
        if len(word) > 8:
            base_score -= random.randint(5, 15)
        elif len(word) > 5:
            base_score -= random.randint(0, 10)
        
        error_type = 'None'
        if base_score < 70:
            error_type = 'Mispronunciation'
        elif base_score < 80:
            error_type = 'Slight mispronunciation'
        
        word_detail = {
            'word': word,
            'accuracy_score': base_score,
            'error_type': error_type,
            'phonemes': []
        }
        
        word_details.append(word_detail)
    
    return word_details

def transcribe_with_azure_sdk(audio_path):
    """
    Use Azure Speech SDK for speech-to-text with pronunciation assessment
    """
    if not AZURE_SDK_AVAILABLE or not AZURE_SPEECH_KEY or not AZURE_SPEECH_REGION:
        raise Exception("Azure Speech SDK not available")
    
    try:
        # Configure speech recognition
        speech_config = speechsdk.SpeechConfig(
            subscription=AZURE_SPEECH_KEY, 
            region=AZURE_SPEECH_REGION
        )
        speech_config.speech_recognition_language = "en-US"
        
        # Configure audio input
        audio_config = speechsdk.audio.AudioConfig(filename=audio_path)
        
        # Create speech recognizer
        speech_recognizer = speechsdk.SpeechRecognizer(
            speech_config=speech_config,
            audio_config=audio_config
        )
        
        # Perform recognition
        result = speech_recognizer.recognize_once()
        
        if result.reason == speechsdk.ResultReason.RecognizedSpeech:
            transcript = result.text.strip()
            print(f"Azure SDK transcription successful: {transcript}")
            
            # Now perform pronunciation assessment on the transcribed text
            return assess_pronunciation_azure(transcript, audio_path)
            
        elif result.reason == speechsdk.ResultReason.NoMatch:
            raise Exception("No speech could be recognized")
        elif result.reason == speechsdk.ResultReason.Canceled:
            cancellation_details = result.cancellation_details
            raise Exception(f"Speech recognition canceled: {cancellation_details.reason}")
        else:
            raise Exception(f"Speech recognition failed: {result.reason}")
            
    except Exception as e:
        print(f"Azure SDK transcription failed: {str(e)}")
        raise Exception(f"Azure SDK transcription failed: {str(e)}")

def analyze_grammar_with_azure(text):
    """
    Simple grammar analysis since Azure Text Analytics is not available in free tier
    """
    try:
        # Simple grammar assessment based on text characteristics
        grammar_score = assess_text_quality(text, 0.85)
        
        return {
            'has_errors': grammar_score < 80,
            'errors': [],
            'corrected_text': text,
            'grammar_score': grammar_score,
            'language_confidence': 0.85,
            'note': 'Basic grammar assessment (Azure Text Analytics not available in free tier)'
        }
            
    except Exception as e:
        print(f"Grammar analysis failed: {str(e)}")
        return {
            'has_errors': False,
            'errors': [],
            'corrected_text': text,
            'grammar_score': 85,
            'note': 'Grammar analysis not available'
        }

def assess_text_quality(text, language_confidence):
    """
    Assess text quality based on various linguistic factors
    """
    import re
    
    # Base score from language confidence
    score = int(language_confidence * 100)
    
    # Adjust based on text characteristics
    words = text.split()
    
    # Check for basic grammar indicators
    if len(words) < 3:
        score -= 10  # Very short utterances
    
    # Check for proper sentence structure
    if not re.search(r'[.!?]$', text.strip()):
        score -= 5  # No proper sentence ending
    
    # Check for capitalization
    if not text[0].isupper() if text else False:
        score -= 5  # No capital letter at start
    
    # Check for repeated words (possible disfluency)
    word_counts = {}
    for word in words:
        word_lower = word.lower()
        word_counts[word_lower] = word_counts.get(word_lower, 0) + 1
    
    repeated_words = sum(1 for count in word_counts.values() if count > 2)
    score -= repeated_words * 3
    
    # Ensure score stays in reasonable range
    return max(60, min(100, score))

def test_azure_basic_transcription(audio_path):
    """
    Test basic Azure transcription with minimal parameters
    """
    try:
        with open(audio_path, 'rb') as audio_file:
            audio_data = audio_file.read()
        
        print(f"Testing basic Azure transcription with {len(audio_data)} bytes")
        
        # Most basic Azure Speech API call
        endpoint = f"https://{AZURE_SPEECH_REGION}.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1"
        
        headers = {
            'Ocp-Apim-Subscription-Key': AZURE_SPEECH_KEY,
            'Content-Type': 'audio/wav',
            'Accept': 'application/json'
        }
        
        params = {
            'language': 'en-US'
        }
        
        print("Making basic Azure API call...")
        response = requests.post(endpoint, headers=headers, params=params, data=audio_data, timeout=30)
        
        print(f"Basic test response status: {response.status_code}")
        print(f"Basic test response: {response.text}")
        
        if response.status_code == 200:
            result = response.json()
            return result
        else:
            return None
            
    except Exception as e:
        print(f"Basic Azure test failed: {e}")
        return None

def analyze_audio_content(audio_path):
    """
    Basic audio content analysis to check if there's actual audio data
    """
    try:
        with open(audio_path, 'rb') as f:
            audio_data = f.read()
        
        # Simple check for audio activity by looking at data variation
        if len(audio_data) < 1000:
            return "Audio file too small"
        
        # Check if the file has variation (not just silence)
        sample_size = min(1000, len(audio_data) // 10)
        samples = []
        for i in range(0, len(audio_data), len(audio_data) // sample_size):
            if i < len(audio_data):
                samples.append(audio_data[i])
        
        # Calculate basic variation
        if len(samples) > 1:
            avg = sum(samples) / len(samples)
            variation = sum(abs(s - avg) for s in samples) / len(samples)
            print(f"Audio variation analysis: avg={avg:.2f}, variation={variation:.2f}")
            
            if variation < 1.0:
                return "Possible silence detected"
            else:
                return "Audio activity detected"
        
        return "Unable to analyze"
        
    except Exception as e:
        return f"Analysis failed: {e}"



@app.route('/api/test_azure', methods=['GET'])
def test_azure_service():
    """
    Test Azure Speech Service connectivity and capabilities
    """
    if not AZURE_SPEECH_KEY or not AZURE_SPEECH_REGION:
        return jsonify({
            'status': 'error',
            'message': 'Azure credentials not configured',
            'azure_speech_key_set': bool(AZURE_SPEECH_KEY),
            'azure_speech_region_set': bool(AZURE_SPEECH_REGION)
        })
    
    try:
        # Test basic Azure Speech Service endpoint
        endpoint = f"https://{AZURE_SPEECH_REGION}.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1"
        
        # Try a simple GET request to check service availability
        headers = {
            'Ocp-Apim-Subscription-Key': AZURE_SPEECH_KEY,
        }
        
        # Test endpoint accessibility
        test_response = requests.get(
            f"https://{AZURE_SPEECH_REGION}.api.cognitive.microsoft.com/",
            headers=headers,
            timeout=10
        )
        
        return jsonify({
            'status': 'testing',
            'azure_region': AZURE_SPEECH_REGION,
            'azure_key_preview': f"{AZURE_SPEECH_KEY[:10]}...{AZURE_SPEECH_KEY[-4:]}",
            'endpoint': endpoint,
            'endpoint_test_status': test_response.status_code,
            'endpoint_test_response': test_response.text[:200],
            'current_issue': 'Azure returns 400 Bad Request for speech recognition',
            'recommendation': 'Check service tier, region support, and pronunciation assessment availability'
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': str(e),
            'azure_region': AZURE_SPEECH_REGION,
            'azure_key_preview': f"{AZURE_SPEECH_KEY[:10]}...{AZURE_SPEECH_KEY[-4:]}",
            'recommendation': 'Azure service may not be accessible or configured correctly'
        })

def assess_pronunciation_azure_advanced(audio_path, topic="Business Communication"):
    """
    Proper Azure pronunciation assessment with content assessment following official pattern
    """
    if not AZURE_SDK_AVAILABLE or not AZURE_SPEECH_KEY or not AZURE_SPEECH_REGION:
        print("Azure Speech SDK not available - using fallback")
        return transcribe_with_openai_whisper_fallback(audio_path)
    
    try:
        print(f"=== AZURE PRONUNCIATION ASSESSMENT ===")
        print(f"Audio file: {audio_path}")
        print(f"Topic: {topic}")
        
        # Create speech config
        speech_config = speechsdk.SpeechConfig(
            subscription=AZURE_SPEECH_KEY, 
            region=AZURE_SPEECH_REGION
        )
        speech_config.speech_recognition_language = "en-US"
        
        # Create audio config from file
        audio_config = speechsdk.audio.AudioConfig(filename=audio_path)
        
        # Create speech recognizer
        recognizer = speechsdk.SpeechRecognizer(
            speech_config=speech_config,
            audio_config=audio_config
        )
        
        # Configure pronunciation assessment with content assessment
        enable_miscue = True
        pron_config = speechsdk.PronunciationAssessmentConfig(
            reference_text="",  # Empty for unscripted assessment
            grading_system=speechsdk.PronunciationAssessmentGradingSystem.HundredMark,
            granularity=speechsdk.PronunciationAssessmentGranularity.Phoneme,
            enable_miscue=enable_miscue
        )
        
        # Enable prosody assessment
        try:
            pron_config.enable_prosody_assessment()
            print("‚úÖ Prosody assessment enabled")
        except Exception as e:
            print(f"‚ö†Ô∏è Prosody assessment failed: {e}")
        
        # Enable content assessment with topic
        try:
            pron_config.enable_content_assessment_with_topic(topic)
            print(f"‚úÖ Content assessment enabled for topic: {topic}")
        except Exception as e:
            print(f"‚ö†Ô∏è Content assessment failed: {e}")
        
        # Apply pronunciation config to recognizer
        pron_config.apply_to(recognizer)
        
        print("Starting single-shot recognition for pronunciation assessment...")
        
        # Use single-shot recognition instead of continuous recognition
        try:
            result = recognizer.recognize_once()
            
            if result.reason == speechsdk.ResultReason.RecognizedSpeech:
                full_transcript = result.text.strip()
                print(f"‚úÖ Recognized: {full_transcript}")
                
                # Get pronunciation assessment result from the single result
                try:
                    pron_result = speechsdk.PronunciationAssessmentResult(result)
                    print(f"üìä Pronunciation assessment retrieved successfully")
                    
                    # Get content assessment result if available
                    content_assessment = None
                    if hasattr(pron_result, 'content_assessment_result') and pron_result.content_assessment_result:
                        content_assessment = pron_result.content_assessment_result
                        print(f"üìã Content scores - Grammar: {content_assessment.grammar_score}, "
                              f"Vocabulary: {content_assessment.vocabulary_score}, "
                              f"Topic: {content_assessment.topic_score}")
                    else:
                        print("‚ö†Ô∏è No content assessment data available")
                    
                    # Build assessment result
                    assessment_result = {
                        'transcript': full_transcript,
                        'assessment_mode': 'azure_advanced_sdk_single_shot'
                    }
                    
                    # Extract word-level pronunciation data
                    word_details = []
                    try:
                        if hasattr(pron_result, 'words') and pron_result.words:
                            print(f"üîç Azure returned {len(pron_result.words)} word-level assessments")
                            for word_obj in pron_result.words:
                                # Extract phoneme details if available
                                phonemes = []
                                if hasattr(word_obj, 'phonemes') and word_obj.phonemes:
                                    phonemes = [{'phoneme': p.phoneme, 'accuracy_score': p.accuracy_score} 
                                              for p in word_obj.phonemes]
                                
                                # Determine error type based on Azure's assessment
                                error_type = "None"
                                if hasattr(word_obj, 'error_type') and word_obj.error_type:
                                    error_type = str(word_obj.error_type)
                                elif word_obj.accuracy_score < 60:
                                    error_type = "Mispronunciation"
                                elif word_obj.accuracy_score < 75:
                                    error_type = "Slight mispronunciation"
                                
                                word_detail = {
                                    'word': word_obj.word,
                                    'accuracy_score': word_obj.accuracy_score,
                                    'error_type': error_type,
                                    'phonemes': phonemes
                                }
                                word_details.append(word_detail)
                                
                                print(f"üìä Word '{word_obj.word}': {word_obj.accuracy_score}% accuracy, error: {error_type}")
                        else:
                            print("‚ö†Ô∏è Azure pronunciation result has no word-level data")
                            print(f"Available attributes on pron_result: {dir(pron_result)}")
                            
                    except Exception as word_error:
                        print(f"‚ùå Error extracting word details: {word_error}")
                    
                    assessment_result['word_details'] = word_details
                    print(f"üìã Final word details count: {len(word_details)}")
                    
                    # Add overall pronunciation scores
                    try:
                        assessment_result['accuracy_score'] = pron_result.accuracy_score
                        assessment_result['fluency_score'] = pron_result.fluency_score
                        assessment_result['pronunciation_score'] = pron_result.pronunciation_score
                        
                        if hasattr(pron_result, 'completeness_score'):
                            assessment_result['completeness_score'] = pron_result.completeness_score
                        
                        if hasattr(pron_result, 'prosody_score'):
                            assessment_result['prosody_score'] = pron_result.prosody_score
                            
                        print(f"üìà Overall scores - Accuracy: {pron_result.accuracy_score}, "
                              f"Fluency: {pron_result.fluency_score}, "
                              f"Pronunciation: {pron_result.pronunciation_score}")
                        
                    except AttributeError as attr_error:
                        print(f"‚ö†Ô∏è Some pronunciation scores not available: {attr_error}")
                        print(f"Available methods: {[method for method in dir(pron_result) if not method.startswith('_')]}")
                    except Exception as score_error:
                        print(f"‚ùå Could not get pronunciation scores: {score_error}")
                    
                    # Add content assessment data if available
                    if content_assessment:
                        assessment_result['content_assessment'] = {
                            'grammar_score': content_assessment.grammar_score,
                            'vocabulary_score': content_assessment.vocabulary_score,
                            'topic_score': content_assessment.topic_score
                        }
                    
                    print(f"‚úÖ Azure single-shot assessment completed successfully")
                    return assessment_result
                    
                except Exception as pron_error:
                    print(f"‚ùå Error getting pronunciation assessment: {pron_error}")
                    # Fall back to basic transcript with empty assessment
                    return {
                        'transcript': full_transcript,
                        'word_details': [],
                        'assessment_mode': 'azure_transcription_only'
                    }
                    
            elif result.reason == speechsdk.ResultReason.NoMatch:
                print("‚ùå No speech could be recognized")
                return transcribe_with_openai_whisper_fallback(audio_path)
            elif result.reason == speechsdk.ResultReason.Canceled:
                cancellation_details = result.cancellation_details
                print(f"‚ùå Speech recognition canceled: {cancellation_details.reason}")
                if cancellation_details.error_details:
                    print(f"‚ùå Error details: {cancellation_details.error_details}")
                return transcribe_with_openai_whisper_fallback(audio_path)
            else:
                print(f"‚ùå Recognition failed with reason: {result.reason}")
                return transcribe_with_openai_whisper_fallback(audio_path)
                
        except Exception as recognition_error:
            print(f"‚ùå Recognition process failed: {recognition_error}")
            return transcribe_with_openai_whisper_fallback(audio_path)
        
    except Exception as e:
        print(f"‚ùå Azure advanced assessment failed: {str(e)}")
        print("Falling back to OpenAI Whisper...")
        return transcribe_with_openai_whisper_fallback(audio_path)

def get_topic_for_conversation(session_id):
    """
    Get the appropriate topic context for pronunciation assessment
    """
    if session_id in sessions:
        topic_mapping = {
            'Business': 'Business Communication',
            'Technology': 'Technology Discussion', 
            'Interview Roleplay': 'Job Interview',
            'Dating Roleplay': 'Social Conversation',
            'Client Meeting': 'Professional Presentation'
        }
        session_topic = sessions[session_id].get('topic', 'Business')
        return topic_mapping.get(session_topic, 'General Conversation')
    return 'General Conversation'

if __name__ == '__main__':
    # Ensure temp directory exists
    os.makedirs(os.path.join(tempfile.gettempdir()), exist_ok=True)
    
    # Start the Flask app
    app.run(debug=True, host='0.0.0.0', port=5000)
